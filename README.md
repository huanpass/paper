# paper
All the papers below are about machine learning system.

## Model Parallelism
PipeDream: Generalized Pipeline Parallelism for DNN Training (*SOSP2019*) [[Paper]](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf) [[Slide]](https://sosp19.rcs.uwaterloo.ca/slides/narayanan.pdf) [[Talk]](https://sosp19.rcs.uwaterloo.ca/videos/D1-S1-P1.mp4)

GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [[Paper]](https://arxiv.org/pdf/1811.06965.pdf) [[Code]](https://github.com/kakaobrain/torchgpipe)

Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform [[Paper]](https://arxiv.org/pdf/1809.02839.pdf)

PipeMare: Asynchronous Pipeline Parallel DNN Training [[Paper]](https://arxiv.org/pdf/1910.05124.pdf)

ElasticPipe: An Efficient and Dynamic Model-Parallel Solution to DNN Training [[Paper]](https://dl.acm.org/citation.cfm?id=3331463)

Horizontal or Vertical? A Hybrid Approach to Large-Scale Distributed Machine Learning [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3322795.3331461?download=true)

XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training [[Paper]](https://arxiv.org/pdf/1911.04610.pdf)

Reduce the Training Time of Neural Networks by Partitioning [[Paper]](https://arxiv.org/pdf/1511.02954.pdf)

STRADS: a distributed framework for scheduled model parallel machine learning (*EuroSys 2016*)[[Paper]](https://dl.acm.org/doi/10.1145/2901318.2901331)

## Beyond data and model parallelism
Beyond Data and Model Parallelism for Deep Neural Networks [[Paper]](https://cs.stanford.edu/~zhihao/papers/sysml19a.pdf)

## Communication Schedule
A Generic Communication Scheduler for Distributed DNN Training Acceleration (*SOSP 2019*) [[Paper]](https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf) [[BytePS]](https://github.com/bytedance/byteps)

TicTac: Accelerating Distributed Deep Learning with Communication Scheduling (SysML 2019) [[Paper]](https://mlsys.org/Conferences/2019/doc/2019/199.pdf)

Distributed Equivalent Substitution Training for Large-Scale Recommender Systems (SysML 2019) [[Paper]](https://arxiv.org/pdf/1909.04823.pdf)

## Resource Schedule
Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters [[Paper]](https://i.cs.hku.hk/~cwu/papers/yhpeng-eurosys18.pdf)

Gandiva: Introspective Cluster Scheduling for Deep Learning (*OSDI 2018*) [[Paper]](https://www.usenix.org/system/files/osdi18-xiao.pdf)

## Network optimization
Optimizing Network Performance for Distributed DNN Training
on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes [[Paper]](https://arxiv.org/pdf/1902.06855.pdf)

## Inference
Optimizing CNN Model Inference on CPUs (*ATC 2019*) [[Paper]](https://www.usenix.org/conference/atc19/presentation/liu-yizhi)

