# paper
All the papers below are about machine learning system.

## Pipeline Model Parallelism
[PipeDream: Generalized Pipeline Parallelism for DNN Training](https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf) (SOSP 2019)

[GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf)

[Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform](https://arxiv.org/pdf/1809.02839.pdf)

[PipeMare: Asynchronous Pipeline Parallel DNN Training](https://arxiv.org/pdf/1910.05124.pdf)

[ElasticPipe: An Efficient and Dynamic Model-Parallel Solution to DNN Training](https://dl.acm.org/citation.cfm?id=3331463)

[XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training](https://arxiv.org/pdf/1911.04610.pdf)


## Beyond data and model parallelism
[Beyond Data and Model Parallelism for Deep Neural Networks](https://cs.stanford.edu/~zhihao/papers/sysml19a.pdf)

## Communication Schedule
[A Generic Communication Scheduler for Distributed DNN Training Acceleration](https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf) 


## Resource Schedule
[Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters](https://i.cs.hku.hk/~cwu/papers/yhpeng-eurosys18.pdf)

## Network optimization
[Optimizing Network Performance for Distributed DNN Training
on GPU Clusters: ImageNet/AlexNet Training in 1.5 Minutes](https://arxiv.org/pdf/1902.06855.pdf)


